{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 2 (Линейная регрессия и KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В каждом пункте от вас потребуется дописать код, необходимый для реализации задания, а так же ответить на ряд вопросов, проанализировав полученные результаты. Просьба отвечать на вопросы развёрнуто, аппелируя к полученным значениям или графикам, ответы вписывать в отдельную ячейку, выбрав для неё тип \"Markdown\". От полноты и качества ответов будет во многом зависеть ваша итоговая оценка."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание выполняется самостоятельно, плагиат будет стандартно наказываться лишением всех баллов за задание.\n",
    "\n",
    "- Максимальная оценка за задание: 10 баллов.\n",
    "- Дата выдачи: 28.2.2018\n",
    "- Срок сдачи: 23:59 13.3.2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.linear_model as lm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Часть 1. Работа с модельными данными"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном пункте требуется разобраться с базовыми техниками регрессионного анализа и сравнить их между собой на данных, чьи характеристики вы уже знаете (т.е., вам их придётся сгенерировать). Тем самым вы сами выявите все преимущества и недостатки используемых методов перед тем, как применять их на реальных данных, где полученные результаты, вообще говоря, без предварительных знаний могут плохо поддаваться объяснению."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как вам уже известно из семинаров и лекций, существуют разные типы регрессии, которые определяют форму и свойства регрессионных кривых. Вы разберётесь с двумя основными способами их задания: трансформацией исходных данных $X$, а так же изменением оптимизируемого функционала $Q(w, b, \\theta)$, где $w, b$ -- настраиваемые параметры линейной регрессии, $\\theta$ -- т.н. гиперпараметры алгоритма построения регрессии, которые не настраиваются по обучающей выборке напрямую, поэтому всегда выделяются отдельно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Генерация данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "От вас потребуется использовать уже знакомый по заданию 1 метод генерации данных для регрессии функций с линейным шумом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_grid(n, D, x_min, x_max):\n",
    "    \"\"\"Генерирует сетку размера n^D x D\"\"\"\n",
    "    xn = np.linspace(x_min, x_max, n)\n",
    "    xn = np.meshgrid(*([xn]*D))\n",
    "    return np.concatenate([xi.reshape((n**D, 1)) for xi in xn], axis=1)\n",
    "\n",
    "def gen_data(n, D, x_min, x_max, f_target, f_noise):\n",
    "    \"\"\"Возвращает аргументы и зашумлённые значения для заданной функции\n",
    "    \n",
    "    Данная функция принимает на вход параметры выборки, которую требуется\n",
    "    сгенерировать, а так же ссылки на функции, которые должны\n",
    "    использоваться при генерации.\n",
    "    \n",
    "    n        -- размер одномерной выборки (совпадает с N при D=1)\n",
    "    D        -- размерность выборки\n",
    "    f_target -- целевая функция, которую будет аппроксимировать регрессия\n",
    "    f_noise  -- функция, которая генерирует шум\n",
    "    \n",
    "    Возвращает сгенерированные данные и ответы на этих данных, а так же\n",
    "    истинные значения функции\n",
    "    \n",
    "    X    -- выборка размера NxD, где N=n^D\n",
    "    y    -- зашумлённые значения целевой функции\n",
    "    y_gt -- истинные значения целевой функции\"\"\"\n",
    "    X = get_grid(n, D, x_min, x_max)\n",
    "    N = X.shape[0]\n",
    "    y_gt = f_target(X)\n",
    "    y = y_gt + f_noise(N)\n",
    "    return X, y, y_gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример генерации выборки для одномерной линейной функции с шумом $y = 2x + 3 + \\epsilon$, где $x$ -- детерминированная переменная, $\\epsilon$ -- шум  из нормального распределения с параметрами $\\mu = 2, \\sigma^2 = 4$, обозначение: $\\epsilon \\sim \\mathcal{N}(2, 4)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_target(X):\n",
    "    return 2*X + 3\n",
    "\n",
    "def f_noise(N):\n",
    "    \"\"\"Обратите внимание, что функция random.normal принимает на вход\n",
    "    стандартное отклонение, т.е. корень из диспресии\"\"\"\n",
    "    return np.random.normal(loc=2, scale=2, size=N).reshape((N, 1))\n",
    "\n",
    "X, y, y_gt = gen_data(10, 1, 0, 10, f_target, f_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Знакомство с библиотеками"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для реализации регрессии вам потребуется пользоваться классами из библиотеки sklearn, в которой собраны практически все основные методы машинного обучения.\n",
    "\n",
    "Базовым методом построения регрессии является линейная регрессия. Для её обучения предлагается использовать класс LinearRegression из модуля sklearn.linear_model\n",
    "\n",
    "Оптимизируемый ей функционал записывается в следующем уже знакомом вам виде: $$ Q(w, b) = \\sum_{i=1}^\\ell (<x_i, w> +~ b - y_i)^2 \\longrightarrow \\min\\limits_{w,b} $$\n",
    "\n",
    "Описание класса: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    "Так же вы познакомитесь с ещё двумя моделями регрессии: Ridge регрессия, которая призвана позволить регрессии работать лучше в случае, когда в признаках присутствовует линейная зависимость, и регрессия Lasso, которая дополнительно осуществляет отбор более информативных с её точки зрения признаков.\n",
    "\n",
    "Функционал для Ridge регрессии:\n",
    "\n",
    "$$ Q(w, b, \\alpha) = \\sum_{i=1}^\\ell (<x_i, w> +~ b - y_i)^2 + \\frac{1}{\\alpha} \\sum_{d=1}^D w_d^2  \\longrightarrow \\min\\limits_{w,b} $$\n",
    "\n",
    "Как видно, выбор оптимальных $w$ и $b$ будет существенно зависеть от выбора $\\alpha$, но при этом сам параметр $\\alpha$ в задаче оптимизации, решаемой по обучающей выборке, не участвует и участвовать не может, поэтому называется гиперпараметром данной модели.\n",
    "\n",
    "Это означает, что Ridge регрессия (как и регрессия Lasso) уже не является решением out of the box, у самого алгоритма присутствуют так называемый гиперпараметр, от выбора которого напрямую будет зависеть качество решения вашей задачи. Такая ситуация крайне типична для машинного обучения. Для настройки параметров существует несколько подходов, и вы с ними познакомитесь позже. Пока что вам предлагается пользоваться регрессиями с параметрами по умолчанию.\n",
    "\n",
    "Функционал для регрессии Lasso:\n",
    "\n",
    "$$ Q(w, b, \\alpha) = \\sum_{i=1}^\\ell (<x_i, w> +~ b - y_i)^2 + \\frac{1}{\\alpha} \\sum_{d=1}^D |w_d|  \\longrightarrow \\min\\limits_{w,b} $$\n",
    "\n",
    "Описание класса Ridge: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "\n",
    "Описание класса Lasso: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n",
    "\n",
    "Ещё при построении регрессионных кривых сложного вида широко используется механизм трансформации признаков, когда вы некоторым способом делаете преобразование вашей выборки, генерируя по ней новую матрицу признаков размера NxD', где D' > D, и уже на новой выборке обучаете линейную регрессию.\n",
    "\n",
    "Например, если в исходной матрице у вас всего два признака a и b, то, преобразовав их в [1, a, b, a^2, ab, b^2], вы сможете, применяя линейную регрессию на новых признаках, получать регрессионные кривые второго порядка.\n",
    "\n",
    "Эти преобразования для любой степени итогового полинома выполняет класс PolynomialFeatures.\n",
    "\n",
    "Описание класса: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn-preprocessing-polynomialfeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Эксперименты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Т.к. эксперименты мы будем проводить на искусственных данных, то для них мы всегда знаем значения незашумлённой функции, поэтому качество работы методов будем оценивать следующим функционалом усреднённой ошибки: $$\\text{err}\\ (y_{pred}, y_{gt}) = \\frac{1}{N} \\sum_{n=1}^N (y_{pred} - y_{gt})^2 $$ где $y_{pred}$ -- значения, предсказанные регрессией, $y_{gt}$ -- истинные значения функции (gt здесь сокращение от groundtruth, т.е. истинная незашумлённая функция)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1 балл) Эксперимент 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируйте зашумлённую выборку для функции $y = x + \\epsilon$, где $x$ принимает значения из равномерной сетки на [0, 3], а $\\epsilon \\sim \\mathcal{N}(0, 1)$. Постройте график зависимости ошибки вашей линейной регрессии от размера выборки. При каком размере выборки регрессионная прямая перестаёт быть визуально отличимой от истинной? Постройте график с истинной прямой и обученной посредством регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример вызова класса, выполняющего линейную регрессию:\n",
    "\n",
    "    LR = lm.LinearRegression()\n",
    "    LR.fit(X_train, y_train)\n",
    "    y_pred = LR.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_target(X):\n",
    "    return X\n",
    "\n",
    "def f_noise(N):\n",
    "    return np.random.normal(loc=0, scale=1, size=N).reshape((N, 1))\n",
    "\n",
    "N_grid = list(range(10, 1000, 10)) # список N, для которых требуется провести эксперимент\n",
    "\n",
    "X, y, y_gt = gen_data(N_grid[-1], 1, 0, 3, f_target, f_noise) # Сгенерированные данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого N в цикле добавьте в список err ошибку регрессии, которую вы обучили на случайных N элементах датасета. Ошибку требуется считать как MSE. Так же для каждого N сохраните обученные классы регрессии в список LRs (в примере вызова класс назван LR). predict делайте на всей выборке X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "err = []\n",
    "LRs = []\n",
    "for N in N_grid:\n",
    "    pass # Place your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код для построения графиков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(10, 1000, 10), err)\n",
    "plt.xlabel(\"N\")\n",
    "plt.ylabel(\"err\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X, y, c=\"black\", alpha=0.1)\n",
    "plt.plot(X, y_gt, linewidth=3)\n",
    "y_pred = LRs[np.argmin(err)].predict(X)\n",
    "plt.plot(X, y_pred, linewidth=3)\n",
    "plt.legend([\"Groundtruth\", \"LR\"], loc=4)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проделайте тоже самое для KNN при n_neighbors=3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "err = []\n",
    "KNNs = []\n",
    "\n",
    "for N in N_grid:\n",
    "    pass # Place your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(10, 1000, 10), err)\n",
    "plt.xlabel(\"N\")\n",
    "plt.ylabel(\"err\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X, y, c=\"black\", alpha=0.1)\n",
    "plt.plot(X, y_gt, linewidth=3)\n",
    "y_pred = KNNs[np.argmin(err)].predict(X)\n",
    "plt.plot(X, y_pred, linewidth=3)\n",
    "plt.legend([\"Groundtruth\", \"KNN\"], loc=4)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Увеличте число соседей до 10 и проделайте тоже самое."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "err = []\n",
    "KNNs = []\n",
    "\n",
    "for N in N_grid:\n",
    "    pass # Place your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(10, 1000, 10), err)\n",
    "plt.xlabel(\"N\")\n",
    "plt.ylabel(\"err\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X, y, c=\"black\", alpha=0.1)\n",
    "plt.plot(X, y_gt, linewidth=3)\n",
    "y_pred = KNNs[np.argmin(err)].predict(X)\n",
    "plt.plot(X, y_pred, linewidth=3)\n",
    "plt.legend([\"Groundtruth\", \"KNN\"], loc=4)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какие выводы можно сделать?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1 балл) Эксперимент 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изучим поведение регрессии с полиномиальными признаками. Для этого будем использовать класс PolynomialFeatures. Код вызова выглядит примерно так:\n",
    "\n",
    "        import sklearn.preprocessing as pp\n",
    "        Poly = pp.PolynomialFeatures(degree=d_new)\n",
    "        X_new = Poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируйте выборку для $y = log \\frac{1}{3} x + \\epsilon$, где $x$ принимает значения из равномерной сетки на  [1, 10], $\\epsilon \\sim \\mathcal{N}(0, 1)$. Рассмотрите три случая: линейный, полиномиальные признаки степени 2 и 5. Для каждого постройте графики, аналогичные графику из эксперимента 1 и сравните их. Какой эффект вы наблюдаете? По какой причине он возникакет? Постройте график истинной кривой, и наилучших среди полиномиальных, которые вы исследовали. Какая из них лучше всего приближает данные и почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as pp\n",
    "\n",
    "def f_target(X):\n",
    "    return np.log(0.3*X)\n",
    "\n",
    "def f_noise(N):\n",
    "    return np.random.normal(loc=0, scale=1, size=N).reshape((N, 1))\n",
    "\n",
    "N_grid = np.arange(10, 1000, 10)\n",
    "X, y, y_gt = gen_data(N_grid[-1], 1, 1, 10, f_target, f_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично предыдущему пункту, обучите в трёх циклах три разные регрессии, сохраняя полученные ошибки и классы в списки err_i и LRs_i. Перед обучением второй и третьих моделей вам требуется преобразовать признаки в полиномиальные, используя указанный в примере класс PolynomialFeatures. Сохраните полученные признаки степени 2 в переменную X_new_2, а признаки степени 5 в переменную X_new_3. Обучение соответственно происходит на выборках (X, y), (X_new_2, y), (X_new_3, y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "err_1 = []\n",
    "LRs_1 = []\n",
    "for N in N_grid:\n",
    "    pass # Place your code here\n",
    "\n",
    "err_2 = []\n",
    "LRs_2 = []\n",
    "pass # Преобразуйте признаки в полиномиальные степени 2\n",
    "for N in N_grid:\n",
    "    pass # Place your code here\n",
    "\n",
    "err_3 = []\n",
    "LRs_3 = []\n",
    "pass # Преобразуйте признаки в полиномиальные степени 5\n",
    "for N in N_grid:\n",
    "    pass # Place your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код для построения всех графиков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(N_grid[:N_grid.size//2], err_1[:N_grid.size//2])\n",
    "plt.plot(N_grid[:N_grid.size//2], err_2[:N_grid.size//2])\n",
    "plt.plot(N_grid[:N_grid.size//2], err_3[:N_grid.size//2])\n",
    "plt.legend([\"LR\", \"Pol deg 2\", \"Pol deg 5\"])\n",
    "plt.xlabel(\"N\")\n",
    "plt.ylabel(\"err\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(N_grid[N_grid.size//2:], err_1[N_grid.size//2:])\n",
    "plt.plot(N_grid[N_grid.size//2:], err_2[N_grid.size//2:])\n",
    "plt.plot(N_grid[N_grid.size//2:], err_3[N_grid.size//2:])\n",
    "plt.legend([\"LR\", \"Pol deg 2\", \"Pol deg 5\"])\n",
    "plt.xlabel(\"N\")\n",
    "plt.ylabel(\"err\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X, y, c=\"black\", alpha=0.1)\n",
    "plt.plot(X, y_gt, linewidth=3)\n",
    "y_pred_1 = LRs_1[np.argmin(err_1)].predict(X)\n",
    "plt.plot(X, y_pred_1, linewidth=3)\n",
    "y_pred_2 = LRs_2[np.argmin(err_2)].predict(X_new_2)\n",
    "plt.plot(X, y_pred_2, linewidth=3)\n",
    "y_pred_3 = LRs_3[np.argmin(err_3)].predict(X_new_3)\n",
    "plt.plot(X, y_pred_3, linewidth = 3)\n",
    "plt.legend([\"Groundtruth\", \"LR\", \"Pol deg 2\", \"Pol deg 5\"], loc=4)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X, y, c=\"black\", alpha=0.1)\n",
    "plt.plot(X, y_gt, linewidth=3)\n",
    "y_pred_2 = LRs_2[np.argmin(err_2)].predict(X_new_2)\n",
    "plt.plot(X, y_pred_2, linewidth=3)\n",
    "y_pred_3 = LRs_3[np.argmin(err_3)].predict(X_new_3)\n",
    "plt.plot(X, y_pred_3, linewidth = 3)\n",
    "plt.legend([\"Groundtruth\", \"Pol deg 2\", \"Pol deg 5\"], loc=4)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1 балл) Эксперимент 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим данные с искусственно внесёнными в них линейными зависимостями. Для наглядности рассмотрим регрессию, которая фактически зависит лишь от одной переменной, но в сгенерированный набор данных внесём ещё несколько признаков, которые будут слабо коррелировать с первым."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируйте зашумлённую выборку для функции $y = x + \\epsilon$, $x$ принимает значения из равномерной сетки на [0, 3], $\\epsilon \\sim \\mathcal{N}(0, 1)$. Добавьте в качестве второго признака $x_2 = 2x + \\delta$, где $\\delta \\sim \\mathcal{N}(0, 0.01)$, а в качестве третьего $x_3 = x_1 + x_2 + \\nu$, где $\\nu \\sim \\mathcal{N}(0, 0.25)$. Постройте графики истинной функции, линейной регрессии и Ridge регрессии, в зависимости от первого столбца данных (при этом саму регрессию нужно обучать по всем трём признакам). Какой эффект вы наблюдаете? Какая регрессия работает лучше и почему? (Для ответа на этот вопрос можете, например, сравнить веса признаков, полученные в случае обычной и Ridge линейных регрессий, параметр класса: coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример вызова класса, выполняющего Ridge регрессию:\n",
    "\n",
    "    Ridge = lm.Ridge()\n",
    "    Ridge.fit(X_train, y_train)\n",
    "    y_pred = Ridge.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_target(X):\n",
    "    return X\n",
    "\n",
    "def f_noise(N):\n",
    "    return np.random.normal(loc=0, scale=1, size=N).reshape((N, 1))\n",
    "\n",
    "\"\"\"Генерация данных\"\"\"\n",
    "N = 1000\n",
    "X, y, y_gt = gen_data(N, 1, 0, 3, f_target, f_noise)\n",
    "delta = np.random.normal(loc=0, scale=0.1, size=N).reshape((N, 1))\n",
    "nu = np.random.normal(loc=0, scale=0.5, size=N).reshape((N, 1))\n",
    "x2 = 2*X + delta\n",
    "x3 = X + x2 + nu\n",
    "X = np.concatenate([X, x2, x3], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично предыдущим пунктам, обучите обычную и Ridge линейные регрессии на выборке (X, y) и получите предсказания для той и другой модели. Сохраните предсказания в переменные y_pred_LR и y_pred_Ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pass # Place your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], y, c=\"black\", alpha=0.1)\n",
    "plt.plot(X[:,0], y_pred_LR, linewidth=3)\n",
    "plt.plot(X[:,0], y_pred_Ridge, linewidth=3)\n",
    "plt.plot(X[:,0], y_gt, linewidth=2)\n",
    "plt.legend([\"LR\", \"Ridge\", \"Groundtruth\"], loc=4)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "График для разности предсказанных ответов и истинных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(X[:,0], y_pred_LR - y_gt)\n",
    "plt.plot(X[:,0], y_pred_Ridge - y_gt)\n",
    "plt.plot(X[:,0], np.zeros(N), linewidth=2)\n",
    "plt.legend([\"LR\", \"Ridge\", \"Groundtruth\"], loc=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.sum((y_pred_LR - y_gt)**2))\n",
    "print(np.sum((y_pred_Ridge - y_gt)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(LR.coef_)\n",
    "print(Ridge.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1 балл) Эксперимент 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим последний предложенный вам вид регрессии: Lasso. Основным его свойством является возможность отбора признаков, в чём вам и придётся убедится. Сгенеририруйте двумерную выборку для зашумлённой функции $y = x_1 + x_2 + \\epsilon$, $x_1$ и $x_2$ образуют равномерную линейную сетку на квадрате [0,1]x[0,1], $\\epsilon \\sim \\mathcal{N}(0, 1)$, и добавьте в неё два линейно зависимых признака с шумом: \n",
    "\n",
    "$$x_3 = 2x_1 + \\delta, \\quad \\delta \\sim \\mathcal{N}(0, 0.01)$$, \n",
    "$$x_4 = 0.5x_1 + x_2 + \\nu, \\quad \\nu \\sim \\mathcal{N}(0, 0.25)$$ \n",
    "\n",
    "После этого запустите на данной выборке регрессию Ridge и Lasso, сравните получаемые веса для признаков. Какие эффекты вы наблюдаете? В чём различие? Как именно в Lasso производится явное уменьшение размерности данных?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример вызова класса, выполняющего регрессию Lasso:\n",
    "\n",
    "    Lasso = lm.Lasso()\n",
    "    Lasso.fit(X_train, y_train)\n",
    "    y_pred = Lasso.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_target(X):\n",
    "    return np.sum(X, axis=1).reshape((X.shape[0], 1))\n",
    "\n",
    "def f_noise(N):\n",
    "    return np.random.normal(loc=0, scale=1, size=N).reshape((N, 1))\n",
    "\n",
    "\"\"\"Генерация выборки\"\"\"\n",
    "n = 30\n",
    "D = 2\n",
    "N = n**D\n",
    "X, y, y_gt = gen_data(n, D, 0, 3, f_target, f_noise)\n",
    "delta = np.random.normal(loc=0, scale=0.1, size=N).reshape((N, 1))\n",
    "nu = np.random.normal(loc=0, scale=0.5, size=N).reshape((N, 1))\n",
    "x3 = 2*X[:,0].reshape((N, 1)) + delta\n",
    "x4 = 0.5*X[:,0].reshape((N, 1)) + X[:,1].reshape((N, 1)) + nu\n",
    "X = np.concatenate([X, x3, x4], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите на выборке регрессии Lasso и Ridge на выборке (X, y), используя вызовы соответствующих классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pass # Place your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(Lasso.coef_)\n",
    "print(Ridge.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. Задача с реальными данными"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной части вам предлагается, используя изученные методы построения регрессии, проанализировать реальный датасет.\n",
    "\n",
    "Будем исследовать данные по преступности в разных штатах США. От вас потребуется по большому количеству признаков, связанных с социальными и экономическими показателями, восстановить целевую переменную: среднее число преступлений на 100 тысяч человек. С полным описанием датасета можно ознакомиться здесь: https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/communities/communities.data\",\n",
    "                   na_values=\"?\", header=None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исходя из описания датасета, первые 5 колонок не являются информативными признаками, убираем их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.iloc[:,5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. (0.5 балла) Вычислите и визуализируйте попарную корреляцию пирсона между всеми признаками.  Как коррелирующие признаки будут влиять на обучение линейной регрессии. Какие выводы можно сделать?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для визуализации можно использовать seaborn.heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pass # Place your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. (0.5 балла) Найдите признаки у которых корреляция с предсказваемым значением максимальна и минимальна. Изобразите на графиках зависимость найденных признаков от предсказываемого значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pass # Place your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. (0.5 балла) Постройте гистограмму распределения предсказываемого значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pass # Place your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно из заголовка датасета, в нём присутствует много пропущенных значений. Рассмотренные нами алгоритмы регрессии не содержат в себе методов обработки пропущенных значений, поэтому нам нужно избавиться от них на этапе предобработки данных.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. (0.5 балла) Замените все пропущенные значения на средние по соответствующим признакам. Сохраните результат в переменную data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pass # Place your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим данные на три выборки: по одной мы будем обучать регрессию, по второй -- гиперпараметры, по третьей -- тестировать качество (Три части необходимы для бонусной части) В базовой части GridSearchCV необходимо делать на (X_train_new, y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.reindex(np.random.permutation(data.index))\n",
    "data_train, data_validate, data_test = np.array_split(data, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Учтём, что целевая переменная -- это последний столбец данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = data_train.iloc[:,:-1]\n",
    "y_train = data_train.iloc[:,-1]\n",
    "X_validate = data_validate.iloc[:,:-1]\n",
    "y_validate = data_validate.iloc[:,-1]\n",
    "X_test = data_test.iloc[:,:-1]\n",
    "y_test = data_test.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_new = np.concatenate([X_train, X_validate], axis=0)\n",
    "y_train_new = np.concatenate([y_train, y_validate], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. (0.5 балла) Обучите обычную линейную регрессию на выборке (X_train_new, y_train_new).  Оцените качество предсказания на данных X_test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество оценивайте, как среднеквадратичное отклонения ваших предсказаний на тестовой выборке от истинных значений: $$\\text{err}\\ (y^{pred}, y^{test}) = \\frac{1}{N} \\sum_{n=1}^N (y^{pred}_n - y^{test}_n)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pass # Place your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. (0.5 балла) Используя sklearn.model_selection.GridSearchCV(), подберите оптимальные значения гиперпараметра alpha для  регрессий Ridge и Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pass # Place your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрите значения полученных весов для регрессии в с лучшим alpha для Ridge и Lasso. В чём их существенная разница? Какие выводы вы можете сделать о структуре данных по регрессионным весам?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исходя из весов регрессии Lasso, отберите наиболее релевантные признаки и обучите две лучшие регрессионные модели только на них. Насколько сильно при этом изменилась средняя ошибка? Что это говорит о структуре данных? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pass # Place your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. (0.5 балла) Обучите KNN на выборке (X_train_new, y_train_new).  Оцените качество предсказания на данных X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pass # Place your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. (0.5 балла) Используя sklearn.model_selection.GridSearchCV(), подберите оптимальные значения параметров n_neighbors и p при выборе  metric='minkowski'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pass # Place your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью какого алгоритма удалось достичь лучшего качества?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
   "#### (2 балла) Напишите отчёт по исследованному вами датасету. Какими он обладает ключевыми свойствами? Какие методы вам пришлось использовать, чтобы его исследовать? Что вы можете сказать об информативности используемых в нём признаков? Какой из опробованных методов вы бы предпочли использовать на нём и почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0.5 балла) Бонусная часть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За выполнение указанного ниже задания вам могут быть начислены дополнительные баллы свыше максимальных 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Реализуйте свмостоятельно функцию, которая будет перебирать значения гиперпараметра по линейной сетке "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Процесс обучения модели будет состоять из двух этапов.\n",
    "\n",
    "Сначала для каждого значения гиперпараметра из заранее заданной сетки мы обучим регрессию на выборке (X_train, y_train) и проверим качество её работы на выборке (X_validate, y_validate). Сохранив полученные значения, среди всех перебранных значений гиперпараметра выберем такое, на котором достигается наименьшая ошибка на выборке (X_validate, y_validate).\n",
    "\n",
    "После этого мы фиксируем значение гиперпараметра, и обучаем итоговый алгоритм на выборке, полученной объединением из X_train и X_validate . Тем самым мы сможем добиться лучшей настройки параметров регресси.\n",
    "\n",
    "В конце концов, для тестирования качества работы нашей модели, мы проверяем её на тестовой выборке (X_test, y_test). Ошибка на этой выборке позволяет нам судить об обобщающей способности нашей модели, т.к. она не участвовала ни в одном этапе обучения. Обобщающая способность -- оценка того, насколько хорошо модель регрессии будет работать на новых данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimizer(X_train, y_train, X_validate, y_validate, \n",
    "              alpha_min, alpha_max, RegressionClass):\n",
    "    alpha_grid = np.linspace(alpha_min, alpha_max, 100)\n",
    "    \n",
    "    #alpha_grid - массив перебираемых значений гиперпараметра\n",
    "    #alpha - найденное оптимальное значение гиперпараметра \n",
    "    #err - массив среднеквадратичных отклонений для каждого из alpha_grid\n",
    "    \n",
    "    return alpha, err, alpha_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример вызова:\n",
    "        \n",
    "        alpha, err, alpha_grid = optimiser(X_train, y_train, \n",
    "                                           X_validate, y_validate,\n",
    "                                           alpha_min, alpha_max,\n",
    "                                           lm.Ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "От вас требуется применить линейную регрессию, а также Ridge и Lasso регрессию, получив оптимальные значения для alpha при помощи функции optimizer, и сравнить полученные среднеквадратичные отклонения ваших предсказаний на тестовой выборке от истинных значений: $$\\text{err}\\ (y^{pred}, y^{test}) = \\frac{1}{N} \\sum_{n=1}^N (y^{pred}_n - y^{test}_n)^2$$\n",
    "\n",
    "Для того чтобы понять, какие значения alpha для регрессий Ridge и Lasso являются оптимальными, вам нужно подобрать параметры alpha_min и alpha_max так, чтобы на графиках зависимости ошибки err от alpha был виден отчётливый минимум. Код для построения графиков указан ниже.\n",
    "\n",
    "Обучите регрессии на выборке (X_train_new, y_train_new)\n",
    "Какая из трёх регрессий работает лучше? Какая хуже? Почему?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделайте вызов функции optimizer для Ridge регрессии на выборках (X_train, y_train), (X_validate, y_validate), воспользовавшись примером её запуска. Получите оптимальное alpha, подобрав параметры alpha_min, alpha_max, как указано в задании. Обучите регрессию на выборке (X_train_new, y_train_new) с полученным параметром alpha, который передаётся при инициализации класса, например:\n",
    "\n",
    "    lm.Ridge(alpha=my_alpha)\n",
    "    \n",
    "и сохраните ответы на тестовой выборке X_test в переменную y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pass # Place your code here\n",
    "\n",
    "print(np.mean((y_pred - y_test)**2))\n",
    "print(data.mean()[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код для построения графиков ошибки на валидационной выборке в зависимости от alpha:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(alpha_grid, err)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично части задания для регресии Ridge, сделайте вызов функции optimizer для регрессии Lasso, обучите её с оптимальным alpha на выборке (X_train_new, y_train_new) и сохраните предсказания для X_test в y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pass # Place your code here\n",
    "\n",
    "print(np.mean((y_pred - y_test)**2))\n",
    "print(data.mean()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(alpha_grid, err)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1.5 балла) Бонусная часть 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё одним способом построения новых признаков по выборке (помимо полиномиальных преобразований) являются так называемые приближения ядер RBF (Radial Basis Function Kernel Approximations). Данный метод используется широко и везде, так как позволяет проводить в данных регрессионные кривые, вообще говоря, практически произвольной сложности. Суть его заключается в том, что он производит некоторое нелинейное преобразование признаков, которое призвано приближать абстрактное пространство бесконечной размерности. Если размерность пространства бесконечна, то в нём можно посредством линейной регрессии провести произвольную разделяющую поверхность -- в этом суть мощности данного метода.\n",
    "\n",
    "У метода существуют два настраиваемых параметра: gamma и n_components. Для объяснения значения первого параметра потребуется подключать достаточно сложную линейную алгебру или привлекать понятие ядровой функции, что мы делать сейчас не будем. Смысл второго параметра -- количество получаемых признаков. Т.к. пространство, которое мы пытаемся приблизить, бесконечной размерности, то, вообще говоря, в n_components можно поставить любое число. Чем больше оно будет, тем лучше наши новые данные будут приближать абстрактное пространство. Данные параметры в любом случае следует лишь перебирать по сетке и выбирать те, на которых меньше всего ошибка. Для того чтобы перебор не оказался слишком большим, положим n_components равным размеру выборки, и будем настраивать лишь gamma. В предложенной задаче это не критично.\n",
    "\n",
    "Ниже приведён код, который уже знакомым вам по функции optimizer образом переберёт по сетке гиперпараметры используемых методов: параметр gamma для RBF, и параметр alpha для Ridge регрессии. Перебор будем осуществлять по всем возможным комбинациям gamma и alpha, выбрав комбинацию с наименьшей ошибкой на валидационной выборке. Обратите внимание, что обучаться мы будем именно по подвыборке признаков, полученных вами в предыдущем задании, т.е. исключая из данных большую часть данных. Интересно, что из этого выйдет?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "обозначим new_features - номера признаков отобранных с помощью Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_shrinked = X_train.iloc[:,new_features]\n",
    "X_validate_shrinked = X_validate.iloc[:,new_features]\n",
    "X_test_shrinked = X_test.iloc[:,new_features]\n",
    "X_train_new_shrinked = X_train_new[:,new_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import kernel_approximation as ka\n",
    "\n",
    "alpha_grid = np.linspace(0.001, 1, 10)\n",
    "gamma_grid = np.linspace(0.001, 1, 10)\n",
    "\n",
    "min_err = +np.inf\n",
    "for alpha in alpha_grid:\n",
    "    for gamma in gamma_grid:\n",
    "        RBF = ka.RBFSampler(gamma=gamma, n_components=X_train.shape[0])\n",
    "        X_train_rbf = RBF.fit_transform(X_train_shrinked)\n",
    "        X_validate_rbf = RBF.transform(X_validate_shrinked)\n",
    "        Ridge = lm.Ridge(alpha=alpha)\n",
    "        Ridge.fit(X_train_rbf, y_train)\n",
    "        y_pred = Ridge.predict(X_validate_rbf)\n",
    "        err = np.mean((y_pred - y_validate)**2)\n",
    "        if err < min_err:\n",
    "            min_err = err\n",
    "            alpha_best = alpha\n",
    "            gamma_best = gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполнив код выше, вы получили две переменные alpha_best и gamma_best с оптимальными значениями гиперпараметров. От вас потребуется обучить на выборке (X_train_new_shrinked, y_train_new) преобразование RBF с параметром gamma_best (параметр n_components заполните так, как указано в примере ниже), а потом обучить на полученных признаках Ridge регрессию с полученным alpha_best и сравнить получившуюся ошибку с ошибой всех предыдущих методов. Какой эффект вы заметили?\n",
    "\n",
    "Для обучения преобразования RBF вам потребуется сначала преобразовать обучающую выборку с помощью метода fit_transform (в вашем случае X_train_new_shrinked), а потом применить обученное преобразование к тестовой выборке (X_test_shrinked)\n",
    "\n",
    "    RBF = ka.RBFSampler(gamma=gamma_best, n_components=X_train_new.shape[0])\n",
    "    X_train = RBF.fit_transform(X_train)\n",
    "    X_test = RBF.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже запишите вызов класса RBF, получите новые признаки для обучающей и тестовой выборки. Постройте Ridge регрессию по выборке (X_train_new_shrinked, y_train_new), найде предсказания для тестовой X_test_shrinked, поместите их в переменную y_pred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pass # Place your code here\n",
    "\n",
    "print(np.mean((y_pred - y_test)**2))\n",
    "print(data.mean()[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
